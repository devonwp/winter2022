---
title: Building large language models
---
Mon Jan 31
: Modeling
  : **Lecture**{: .label .label-purple } **Discussion**{: .label .label-green }
: *Percy Liang*
: 1. Tokenization
  1. RNNs, Transformers
: Required reading:
  - TBD

Wed Feb 2
: Training
  : **Lecture**{: .label .label-purple } **Discussion**{: .label .label-green }
: *Percy Liang*
: 1. Objective functions
  1. Stability
  1. Debugging
: Required reading:
  - TBD

Mon Feb 7
: Scaling laws
  : **Lecture**{: .label .label-purple } **Discussion**{: .label .label-green }
: *Tatsunori Hashimoto*
: 1. Scaling laws
: Required reading:
  - [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf)

Wed Feb 9
: Parallelism
  : **Lecture**{: .label .label-purple } **Discussion**{: .label .label-green }
: *Christopher RÃ©*
: 1. Data parallelism
  1. Model parallelism
  1. Pipeline parallelism
: Required reading:
  - TBD

Mon Feb 14
: Modular architectures
  : **Lecture**{: .label .label-purple } **Discussion**{: .label .label-green }
: *Percy Liang*
: 1. Mixture of experts
  1. Memory-augmented (retrieval) models
: Required reading:
  - TBD

Wed Feb 16
: Adaptation
  : **Lecture**{: .label .label-purple } **Discussion**{: .label .label-green }
: *Michael Xie*
: 1. Probing
  1. Fine-tuning
  1. Lightweight fine-tuning
: Required reading:
  - TBD

Mon Feb 21
: no class (Presidents' Day)

Wed Feb 23
: Environmental impact
  : **Lecture**{: .label .label-purple } **Discussion**{: .label .label-green }
: *Percy Liang*
: 1. Training and inference costs
  1. Carbon emissions
: Required reading:
  - TBD
