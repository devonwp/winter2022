---
layout: page
grand_parent: Lectures
parent: Building
title: Parallelism
nav_order: 4.4
usemathjax: true
---
*UNDER CONSTRUCTION*

## Data parallelism

## Further reading

- [Efficient large-scale language model training on GPU clusters using megatron-LM](https://arxiv.org/pdf/2104.04473.pdf). *D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, J. Bernauer, Bryan Catanzaro, Amar Phanishayee, M. Zaharia*. SC 2021.
- [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/pdf/2006.16668.pdf). *Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, M. Krikun, Noam M. Shazeer, Z. Chen*. ICLR 2020.
- [Towards Crowdsourced Training of Large Neural Networks using Decentralized Mixture-of-Experts](https://arxiv.org/pdf/2002.04013.pdf). *Max Ryabinin, Anton I. Gusev*. NeurIPS 2020.
- [ZeRO: Memory Optimization Towards Training A Trillion Parameter Models](https://arxiv.org/pdf/1910.02054.pdf). *Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He*. SC 2019.
